%       Syntax from following sources
%               https://www.sharelatex.com/learn/Sections_and_chapters
%               https://www.sharelatex.com/learn/Table_of_contents
%               http://www.bibtex.org/Using/
%               https://www.economics.utoronto.ca/osborne/latex/BIBTEX.HTM
%               https://www.latex-tutorial.com/tutorials/beginners/latex-bibtex/
%               http://tex.stackexchange.com/questions/205/what-graphics-packages-are-there-for-creating-graphics-in-latex-documents
% 				http://www.emerson.emory.edu/services/latex/latex_132.html

% Main stuff
%\documentclass[a4paper,10pt]{article}
\documentclass{report}
\usepackage[utf8]{inputenc}

%% Math Packages
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}

%% Date Time Pakcages
\usepackage[USenglish]{babel}
\usepackage[nodayofweek,level]{datetime}
\usepackage[margin=0.5in]{geometry}

%% Formatting Packages
\usepackage{indentfirst}

%% Citation Packages
\usepackage{cite}
\usepackage{hyperref}

%% Tables Packages
\usepackage{array,booktabs,ragged2e}

%% SVG Packages
\usepackage{svg}
%\setsvg{
%	inkscape = inkscape -z -D 
%	% conversion options for svg package, export drawing instead of page
%}

%% Commands Section
%\newcommand{\logentry}[4]{ \selectlanguage{USenglish} \formatdate{#2}{#1}{#3}  & {#4}  \\ \hline}
\newcommand{\cW}[1]{^{W}_{C}{#1}}
\newcommand{\tR}[0]{\ensuremath{^{A}}}
\newcommand{\bR}[0]{\ensuremath{_{A}}}
\newcommand{\tL}[0]{\ensuremath{^{B}}}
\newcommand{\bL}[0]{\ensuremath{_{B}}}
\newcommand{\tT}[0]{\ensuremath{^{\intercal}}}
\newcommand{\rL}[0]{\ensuremath{{\tL\bR}}}
\newcommand{\xL}[0]{\ensuremath{{\tL\mathbf{x}}}}
\newcommand{\xR}[0]{\ensuremath{{\tR\mathbf{x}}}}
\newcommand{\hxL}[0]{\ensuremath{{\tL\tilde{\mathbf{x}}}}}
\newcommand{\hxR}[0]{\ensuremath{{\tR\tilde{\mathbf{x}}}}}
\newcommand{\rLM}[0]{\ensuremath{{\tL\bR}\mathbf{M}}}
\newcommand{\rLR}[0]{\ensuremath{{\tL\bR}\mathbf{R}}}
\newcommand{\rLt}[0]{\ensuremath{{\tL\bR}\mathbf{t}}}
\newcommand{\skewsym}[1]{\ensuremath{\left[#1\right]_{\times}}}
\newcommand{\sR}[0]{\ensuremath{^{A}\mathbb{S}}}
\newcommand{\sL}[0]{\ensuremath{^{B}\mathbb{S}}}
\newcommand{\lR}[0]{\ensuremath{{^{A}l}}}
\newcommand{\lL}[0]{\ensuremath{{^{B}l}}}
%% ColumnTypes Section
\newcolumntype{R}[1]{>{\RaggedLeft\arraybackslash}p{#1}}

\title{Thesis or Article}
\author{JeffGWood@mavs.uta.edu}
\date{\today}

\pdfinfo{%
  /Title    (Thesis or Article)
  /Author   (Jeff Wood)
  /Creator  ()
  /Producer ()
  /Subject  ()
  /Keywords ()
}

\begin{document}
\Huge
\maketitle
\large
\newpage
\tableofcontents
\newpage
%\Huge\center\textbf{Geometric Formulas for \newline Computer Vision \& \newline Computer Graphics}\newline

%\large Written by Jeff Wood in LaTeX environment\newline
%\large Updated 2016/03/06 \newline

%\normalsize
%\flushleft
\chapter{Introduction}
%\begin{flushleft}
\par Through the development of applications such as augmented and virtual reality, object / scene reconstruction and visual effects, the process of generating images from an arbitrary vantage point can be found in a variety of applications. In this Thesis (or Article) I will discuss various methods for Image Creation from an arbitraryvantage point, which can be accomplished by two main methodologies of Geometric Construction and Image Synthesis. While both methods use stereo correspondance of multiple images, they differ in the way information is stored and used.
\par Geometric Construction (GC) contains information about the real-world spatial properties (Coordinates in space, Color), thus viewing results are non-constrained in vantage point. Image Synthesis (IS) relies on image properties (pixel displacement) and is thus viewing results are imited in the possible vantage points.
\newpage
\section*{Symbols and Notation}
\begin{tabular}{R{2cm} p{14cm}}
\toprule
\multicolumn{1}{l}{\textbf{Symbol}} & \textbf{Description} \\
\midrule
$\mathbf{v}$ & \textit{Vectors} in \textit{lowercase} bold\\
$\mathbf{M}$ & \textit{Matrices} in \textit{uppercase} bold\\ 
$\mathbf{u}$ & Image coordinate\\
$\mathbf{\tilde{u}}$ & Image coordinate (expressed \textit{homogeneously})\\
$\mathbf{x}$ & Spatial coordinate\\
$\mathbf{\tilde{x}}$ & Spatial coordinate (expressed \textit{homogeneously})\\
$^{A}{\mathbf{x}}$ & Spatial coordinate in reference frame \textit{A} \\
$^{A}{\mathbf{\tilde{x}}}$ & Spatial coordinate (expressed \textit{homogeneously}) in reference frame {A} \\
$^{C}_{B}\mathbf{\tilde{M}}$ & Change from of reference frame \textit{B} to reference frame \textit{C}\\
$s$ & Normalizing factor applied to \textit{homogeneous} vector so last element becomes equal to 1\\
$^{D}\mathbb{S}$ & Spatial reference frame \textit{D}\\
$\skewsym{\mathbf{x}}$ & Skew-symmetric matrix version of vector $\mathbf{x}$ used as \textit{left}-operand in the \textit{cross}-product such that $\skewsym{\mathbf{x}}\cdot\mathbf{y}=\mathbf{x}\times\mathbf{y}$\\
$l$ & Epipolar line\\
\bottomrule
\end{tabular}
\newpage
%\end{flushleft}

\chapter{Backround}
\par 
Oridinarily, real-world data contains 3-dimensions. 
Because standard images only include 2-dimensional data, information regarding depth is lost (i.e. it is often difficult to judge distance from a single image without visual cues). 
\textit{Stereovision} attempts to resolved this by finding the same point in both \textit{stereoscopic} images (known as a \textit{corresponding point}), and recovering the depth information.
An elementry example of this occurs in stereoscopic images with relatively low distance between cameras (i.e they are righht next to each other). 
Objects that are \textit{farther} away from the observer occur closer together in the stereo images, whereas objects \textit{closer} to the camera appear appear farther appart in the stereo-images.

\subsection*{Change of Reference}
\par
Each view from a pair of stereo-images encompasses its own \textit{frame of reference} (i.e. the directions of \textit{forward} or \textit{backward} are unique to image and may differe considerably depending on camera displacement).
This requires expressing points from different frames of reference (traditionally referred to \textit{left} and \textit{right}) in a single reference frame. 
As such it is necessary to be able to express coordinates in a given reference frame in any other reference frame.
\par
Coordinates given in $\xR$ can be expressed in $\xL$ by the geometric transformation:
\renewcommand{\arraystretch}{1.5}
\par
\begin{align*}
	\xL = \rLR \cdot \xR + \rLt
\end{align*}
or
\begin{align*}
	\hxL &=
	\left[\begin{array}{c|c}
		\rL\textbf{R} & \rL\textbf{t} \\\hline
		0 & 1 \\
	\end{array}\right]
	\cdot\hxR \\
	&\\
	&= \rLM\cdot\hxR\\
\end{align*}
where $\rLM$ is also the geometric transformation necessary to transform $\sL$ into $\sR$. 
\par
Withough calculating any new quantities, rearranging allows us to express coordinates in $\xL$ in the $\xR$ reference frame as:
\par
\begin{equation*}
	{\rLR\tT}\cdot (\xL - \rLt ) = \xR
\end{equation*}
and similarly transforms $\sR$ into $\sL$.

\subsection*{Epipolar constraint}

\subsection*{Essential Matrix}
\par When coordinates from a reference frame are expressed as \textit{normalized image coordinate}s the range of possible NIC values in the corresponding image are given by the \textit{epipolar} constraint 

\subsection*{Intrinsic Calibration Matrix}

\subsection*{Essential Matrix}
%Longuet-Higgins' Fundamental Matix ~\cite{Longuet-Higgins}

%\bibliography{citations}{}
%\bibliographystyle{plain}

\chapter{Point Interpolation}
\par
Pixels from image \textit{a} and image \textit{b} can be used to create a new images. This is done by interpolating the pixel positions ($\mathbf{p}^{a}_{uv}$ and $\mathbf{p}^{b}_{uv}$) of corresponding points between frames. Because not all pixels are established as corresponding points, pixel correspondances \textit{between} corresponding points ($\mathbf{p}_{uv}$) are calculated through bi-linear interpolation of 4 established corresponding points: 
\begin{equation*}
\mathbf{P}_{uv} = 
\mathbf{P}_{00}\cdot (1-u)\cdot (1-v)+\mathbf{P}_{10}\cdot u \cdot (1-v)+
\mathbf{P}_{01}\cdot (1-u)\cdot v +\mathbf{P}_{11}\cdot u \cdot v
\end{equation*}
This is done through the following series of linear equations
%textit{uv} calculation using 
\begin{equation*}\begin{split}
x_{uv} &= 
\begin{bmatrix}u & 1\end{bmatrix}
\begin{bmatrix}-1 & 1\\ 1 & 0\\\end{bmatrix}
\begin{bmatrix}x_{00} & x_{01} \\ x_{10} & x_{11}\\\end{bmatrix}
\begin{bmatrix}-1 & 1\\ 1 & 0\\\end{bmatrix}
\begin{bmatrix}v \\ 1\\\end{bmatrix}\\
y_{uv} &= 
\begin{bmatrix}u & 1\end{bmatrix}
\begin{bmatrix}-1 & 1\\ 1 & 0\\\end{bmatrix}
\begin{bmatrix}y_{00} & y_{01} \\ y_{10} & y_{11}\\\end{bmatrix}
\begin{bmatrix}-1 & 1\\ 1 & 0\\\end{bmatrix}
\begin{bmatrix}v \\ 1\\\end{bmatrix}\\
\end{split}\end{equation*}
or as a single matrix equation of 
\begin{equation*}
\begin{split}
\begin{bmatrix}x_{uv} & 0 \\ 0 & y_{uv}\\\end{bmatrix}
&=
\begin{bmatrix}\mathbf{u} & \mathbf{0} \\ \mathbf{0} & \mathbf{u} \end{bmatrix}^{T}
\begin{bmatrix}\mathbf{M} & \mathbf{0} \\ \mathbf{0} & \mathbf{M} \end{bmatrix}^{T}
\begin{bmatrix}\mathbf{X} & \mathbf{0} \\ \mathbf{0} & \mathbf{Y} \end{bmatrix}
\begin{bmatrix}\mathbf{M} & \mathbf{0} \\ \mathbf{0} & \mathbf{M} \end{bmatrix}
\begin{bmatrix}\mathbf{v} & \mathbf{0} \\ \mathbf{0} & \mathbf{v} \end{bmatrix}
\end{split}
\end{equation*}
where
\begin{equation*}
\begin{split}
%
\mathbf{u} =\begin{bmatrix}u \\ 1\\\end{bmatrix}
\text{, }
%
\mathbf{v} =\begin{bmatrix}v \\ 1\\\end{bmatrix}
\text{, }
%
\mathbf{X} =\begin{bmatrix}x_{00} & x_{01} \\ x_{10} & x_{11}\\\end{bmatrix}
\text{, }
%
\mathbf{Y} =\begin{bmatrix}y_{00} & y_{01} \\ y_{10} & y_{11}\\\end{bmatrix}
\text{, and }
%
\mathbf{M} =\begin{bmatrix}-1 & 1 \\ 1 & 0\\\end{bmatrix}
\end{split}
\end{equation*}
\begin{figure}[htbp]
	\centering
%	\includesvg{BiLinear_Point_Correspondance}
	\includegraphics[scale=0.25]{BiLinear_Point_Correspondance}
	\caption{Bi-Linear Point Correspondance}
\end{figure}


	
\chapter{Process}
The system in question contains 3 main components
\begin{enumerate}
	\item Image Acquisition System
	\begin{itemize}
		\item Webcam / Kinect set-up
		\item If Webcam should also contain Image-Processing module for:
		\begin{itemize}
			\item Feature Identification
			\item Point-correspondance
			\item Sub-Pixel interpolation
		\end{itemize}
	\end{itemize}
	\item Point Cloud Processing
	\begin{itemize}
		\item Should take inputs
		\item Should produce point-clouds as one of the output
		\item (Possible) Options for Surface Reconstruction include:
		\begin{itemize}
			\item Calculation of surface Normal through PCA
			\item Mesh construction through Delaunay trianglulation
			\item Parametrization of Bezier surface through linear-least squares.
		\end{itemize}

	\end{itemize}

\end{enumerate}

\begin{equation*}
%\cAij{i}{j}
\end{equation*}


\end{document}
